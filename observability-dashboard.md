# üöÄ Observability Dashboard - Complete Setup & Configuration Guide

## üìã Table of Contents

1. [Dashboard Overview](#dashboard-overview)
2. [Architecture & Components](#architecture--components)
3. [Dashboard Setup](#dashboard-setup)
4. [Metrics & Visualizations](#metrics--visualizations)
5. [Alerting Configuration](#alerting-configuration)
6. [Troubleshooting](#troubleshooting)
7. [Advanced Features](#advanced-features)

---

## üéØ Dashboard Overview

This guide provides comprehensive setup instructions for creating production-ready observability dashboards that leverage the OpenTelemetry data collected from our multi-tenant document management system. The dashboards provide real-time visibility into system performance, health, and business metrics.

### üåü Dashboard Capabilities

- **üìä Real-time Performance Monitoring**: Response times, throughput, and error rates
- **üîç Distributed Tracing Visualization**: Request flows across all services
- **üìà Business Metrics**: Client usage, document processing, and capacity planning
- **üö® Proactive Alerting**: Automated issue detection and notification
- **üì± Multi-platform Access**: Web dashboards, mobile apps, and API access

---

## üèóÔ∏è Architecture & Components

### Complete Observability Stack Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        DA[Document API<br/>FastAPI + OTEL]
        DS[Data Store<br/>FastAPI + SQLAlchemy + OTEL]
        PG[(PostgreSQL<br/>Database)]
    end
    
    subgraph "Collection Layer"
        OC[OTEL Collector<br/>Central Hub]
        B[Batch Processing]
        F[Filtering & Enrichment]
    end
    
    subgraph "Storage Layer"
        TS[Time Series DB<br/>Prometheus/InfluxDB]
        LS[Log Storage<br/>Elasticsearch]
        TR[Trace Storage<br/>Jaeger/Tempo]
    end
    
    subgraph "Visualization Layer"
        G[Grafana<br/>Dashboards]
        A[AlertManager<br/>Notifications]
        API[Grafana API<br/>Automation]
    end
    
    subgraph "Notification Layer"
        S[Slack<br/>Team Alerts]
        E[Email<br/>Escalation]
        P[PagerDuty<br/>On-call]
        W[Webhook<br/>Custom Integrations]
    end
    
    DA --> OC
    DS --> OC
    PG --> OC
    
    OC --> B
    B --> F
    F --> TS
    F --> LS
    F --> TR
    
    TS --> G
    LS --> G
    TR --> G
    
    G --> A
    A --> S
    A --> E
    A --> P
    A --> W
    
    style OC fill:#ff6b6b
    style G fill:#4ecdc4
    style A fill:#45b7d1
    style TS fill:#96ceb4
```

### Data Flow Architecture

```mermaid
flowchart LR
    subgraph "Services"
        DA[Document API]
        DS[Data Store]
    end
    
    subgraph "OTEL Collector"
        R[Receivers<br/>OTLP gRPC/HTTP]
        P[Processors<br/>Batch, Filter, Enrich]
        E[Exporters<br/>Multiple Destinations]
    end
    
    subgraph "Storage Backends"
        P1[Prometheus<br/>Metrics]
        I1[InfluxDB<br/>Metrics + Logs]
        E1[Elasticsearch<br/>Logs + Traces]
        J1[Jaeger<br/>Traces]
    end
    
    subgraph "Dashboard Platform"
        G[Grafana<br/>Unified View]
        K[Kibana<br/>Log Analysis]
        J2[Jaeger UI<br/>Trace Analysis]
    end
    
    DA -->|OTLP| R
    DS -->|OTLP| R
    
    R --> P
    P --> E
    
    E --> P1
    E --> I1
    E --> E1
    E --> J1
    
    P1 --> G
    I1 --> G
    E1 --> K
    J1 --> J2
    
    style R fill:#ff6b6b
    style P fill:#4ecdc4
    style E fill:#45b7d1
    style G fill:#96ceb4
```

### Dashboard Component Hierarchy

```mermaid
graph TD
    subgraph "Dashboard Structure"
        M[Main Dashboard<br/>System Overview]
        S[Service Dashboards<br/>Per-service Details]
        B[Business Dashboards<br/>Client Metrics]
        A[Alerting Dashboards<br/>Issue Management]
    end
    
    subgraph "Main Dashboard Panels"
        P1[System Health<br/>Overall Status]
        P2[Performance Overview<br/>Response Times]
        P3[Error Rates<br/>Success/Failure]
        P4[Resource Usage<br/>CPU, Memory, Disk]
    end
    
    subgraph "Service Dashboard Panels"
        SP1[Request Volume<br/>RPS by Service]
        SP2[Latency Distribution<br/>P50, P95, P99]
        SP3[Error Breakdown<br/>By Endpoint & Status]
        SP4[Database Performance<br/>Query Metrics]
    end
    
    M --> P1
    M --> P2
    M --> P3
    M --> P4
    
    S --> SP1
    S --> SP2
    S --> SP3
    S --> SP4
    
    M --> S
    S --> B
    B --> A
    
    style M fill:#ff6b6b
    style S fill:#4ecdc4
    style B fill:#45b7d1
    style A fill:#96ceb4
```

---

## üöÄ Dashboard Setup

### 1. **Infrastructure Setup**

#### **Docker Compose Configuration**

```mermaid
graph TD
    subgraph "Infrastructure Services"
        G[Grafana<br/>Port 3000]
        P[Prometheus<br/>Port 9090]
        A[AlertManager<br/>Port 9093]
        E[Elasticsearch<br/>Port 9200]
        K[Kibana<br/>Port 5601]
        J[Jaeger<br/>Port 16686]
    end
    
    subgraph "Application Services"
        DA[Document API<br/>Port 80]
        DS[Data Store<br/>Port 8001]
        OC[OTEL Collector<br/>Port 4317]
        PG[PostgreSQL<br/>Port 5432]
    end
    
    subgraph "Network Configuration"
        N1[Frontend Network<br/>Public Access]
        N2[Backend Network<br/>Internal Services]
        N3[Monitoring Network<br/>Metrics & Logs]
    end
    
    G --> N1
    P --> N3
    A --> N3
    E --> N3
    K --> N1
    J --> N1
    
    DA --> N1
    DS --> N2
    OC --> N3
    PG --> N2
    
    style G fill:#ff6b6b
    style P fill:#4ecdc4
    style A fill:#45b7d1
    style E fill:#96ceb4
```

#### **Environment Configuration**

```bash
# Grafana Configuration
GF_SECURITY_ADMIN_PASSWORD=admin123
GF_USERS_ALLOW_SIGN_UP=false
GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource

# Prometheus Configuration
PROMETHEUS_RETENTION_TIME=15d
PROMETHEUS_STORAGE_TSDB_RETENTION_SIZE=50GB

# Elasticsearch Configuration
ELASTIC_PASSWORD=elastic123
ELASTIC_HEAP_SIZE=1g
ELASTIC_MAX_MEMORY=1g

# Jaeger Configuration
JAEGER_SAMPLING_TYPE=probabilistic
JAEGER_SAMPLING_PARAM=0.1
```

### 2. **Grafana Dashboard Setup**

#### **Dashboard Import Process**

```mermaid
flowchart TD
    subgraph "Dashboard Creation"
        T1[Create Dashboard<br/>Basic Structure]
        T2[Add Data Sources<br/>Prometheus, Elasticsearch]
        T3[Create Panels<br/>Metrics Visualization]
        T4[Configure Variables<br/>Dynamic Filtering]
        T5[Set Refresh Rates<br/>Real-time Updates]
    end
    
    subgraph "Panel Configuration"
        P1[Time Series<br/>Line Charts]
        P2[Stat Panels<br/>Single Values]
        P3[Table Panels<br/>Detailed Data]
        P4[Heatmap Panels<br/>Distribution]
        P5[Log Panels<br/>Text Display]
    end
    
    subgraph "Data Source Types"
        DS1[Prometheus<br/>Metrics]
        DS2[Elasticsearch<br/>Logs]
        DS3[Jaeger<br/>Traces]
        DS4[InfluxDB<br/>Time Series]
    end
    
    T1 --> T2
    T2 --> T3
    T3 --> T4
    T4 --> T5
    
    T3 --> P1
    T3 --> P2
    T3 --> P3
    T3 --> P4
    T3 --> P5
    
    T2 --> DS1
    T2 --> DS2
    T2 --> DS3
    T2 --> DS4
    
    style T1 fill:#ff6b6b
    style T3 fill:#4ecdc4
    style T2 fill:#45b7d1
    style P1 fill:#96ceb4
```

#### **Dashboard JSON Configuration**

```json
{
  "dashboard": {
    "title": "Document Management System - Overview",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{service}} - {{method}} {{path}}"
          }
        ]
      },
      {
        "title": "Response Time P95",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{service}} - {{path}}"
          }
        ]
      }
    ]
  }
}
```

---

## üìä Metrics & Visualizations

### **Key Performance Indicators (KPIs)**

#### **System Health Metrics**

```mermaid
graph LR
    subgraph "Health Metrics"
        H1[Service Status<br/>healthy/degraded/unhealthy]
        H2[Health Check Latency<br/>response time]
        H3[Component Status<br/>file_system, database]
        H4[Uptime Percentage<br/>availability]
    end
    
    subgraph "Health Thresholds"
        T1[Healthy<br/>< 500ms]
        T2[Degraded<br/>500ms - 2s]
        T3[Unhealthy<br/>> 2s]
        T4[Critical<br/>service down]
    end
    
    subgraph "Health Actions"
        A1[Monitor<br/>watch trends]
        A2[Alert<br/>notify team]
        A3[Escalate<br/>page on-call]
        A4[Auto-remediate<br/>restart services]
    end
    
    H1 --> T1
    H2 --> T2
    H3 --> T3
    H4 --> T4
    
    T1 --> A1
    T2 --> A2
    T3 --> A3
    T4 --> A4
    
    style H1 fill:#ff6b6b
    style T1 fill:#4ecdc4
    style A1 fill:#45b7d1
```

#### **Performance Metrics Structure**

```mermaid
graph TD
    subgraph "Request Metrics"
        RM1[http_requests_total<br/>request volume]
        RM2[http_request_duration_seconds<br/>response time]
        RM3[http_errors_total<br/>error count]
        RM4[http_request_size_bytes<br/>payload size]
    end
    
    subgraph "Database Metrics"
        DM1[database_operations_total<br/>query volume]
        DM2[database_operation_duration_seconds<br/>query time]
        DM3[database_connection_pool<br/>pool usage]
        DM4[database_transactions<br/>transaction count]
    end
    
    subgraph "Business Metrics"
        BM1[documents_uploaded_total<br/>upload volume]
        BM2[client_activity_total<br/>client usage]
        BM3[file_processing_duration<br/>processing time]
        BM4[llm_summarization_duration<br/>AI processing]
    end
    
    subgraph "Resource Metrics"
        RES1[cpu_usage_percent<br/>CPU utilization]
        RES2[memory_usage_bytes<br/>memory usage]
        RES3[disk_usage_percent<br/>disk utilization]
        RES4[network_io_bytes<br/>network traffic]
    end
    
    RM1 --> RM2
    RM2 --> RM3
    RM3 --> RM4
    
    DM1 --> DM2
    DM2 --> DM3
    DM3 --> DM4
    
    BM1 --> BM2
    BM2 --> BM3
    BM3 --> BM4
    
    RES1 --> RES2
    RES2 --> RES3
    RES3 --> RES4
    
    style RM1 fill:#ff6b6b
    style DM1 fill:#4ecdc4
    style BM1 fill:#45b7d1
    style RES1 fill:#96ceb4
```

### **Dashboard Panel Examples**

#### **Request Volume Panel**

```mermaid
graph LR
    subgraph "Request Volume Visualization"
        V1[Time Series Chart<br/>Line Graph]
        V2[Request Count<br/>Total per minute]
        V3[Service Breakdown<br/>By service name]
        V4[Endpoint Breakdown<br/>By API path]
    end
    
    subgraph "Data Sources"
        DS1[Prometheus Query<br/>rate(http_requests_total[1m])]
        DS2[Grouping<br/>by service, method, path]
        DS3[Time Range<br/>Last 24 hours]
        DS4[Refresh Rate<br/>Every 10 seconds]
    end
    
    subgraph "Panel Configuration"
        C1[Y-Axis<br/>Requests per second]
        C2[X-Axis<br/>Time series]
        C3[Legend<br/>Service + Endpoint]
        C4[Thresholds<br/>Warning/Error levels]
    end
    
    V1 --> DS1
    V2 --> DS2
    V3 --> DS3
    V4 --> DS4
    
    DS1 --> C1
    DS2 --> C2
    DS3 --> C3
    DS4 --> C4
    
    style V1 fill:#ff6b6b
    style DS1 fill:#4ecdc4
    style C1 fill:#45b7d1
```

#### **Error Rate Panel**

```mermaid
graph TD
    subgraph "Error Rate Calculation"
        E1[Total Errors<br/>http_errors_total]
        E2[Total Requests<br/>http_requests_total]
        E3[Error Rate<br/>errors / total * 100]
        E4[Error Breakdown<br/>by status code]
    end
    
    subgraph "Error Categories"
        EC1[4xx Errors<br/>Client errors]
        EC2[5xx Errors<br/>Server errors]
        EC3[Timeout Errors<br/>Request timeouts]
        EC4[Connection Errors<br/>Network issues]
    end
    
    subgraph "Error Thresholds"
        ET1[Warning<br/>> 1% error rate]
        ET2[Critical<br/>> 5% error rate]
        ET3[Emergency<br/>> 10% error rate]
    end
    
    E1 --> E2
    E2 --> E3
    E3 --> E4
    
    E4 --> EC1
    E4 --> EC2
    E4 --> EC3
    E4 --> EC4
    
    E3 --> ET1
    E3 --> ET2
    E3 --> ET3
    
    style E1 fill:#ff6b6b
    style E4 fill:#4ecdc4
    style ET1 fill:#45b7d1
```

---

## üö® Alerting Configuration

### **Alerting Architecture**

#### **Alert Flow Diagram**

```mermaid
flowchart TD
    subgraph "Alert Sources"
        P[Prometheus<br/>Metrics Alerts]
        G[Grafana<br/>Dashboard Alerts]
        L[Log Aggregation<br/>Log-based Alerts]
        H[Health Checks<br/>Service Health]
    end
    
    subgraph "Alert Processing"
        AR[Alert Rules<br/>Thresholds & Conditions]
        AG[Alert Grouping<br/>Similar Alerts]
        AI[Alert Inhibition<br/>Prevent Duplicates]
        AS[Alert Silencing<br/>Maintenance Windows]
    end
    
    subgraph "Alert Routing"
        AM[AlertManager<br/>Central Router]
        AG1[Group by Service<br/>Service-specific routing]
        AG2[Group by Severity<br/>Priority-based routing]
        AG3[Group by Time<br/>Time-based routing]
    end
    
    subgraph "Notification Channels"
        NC1[Slack<br/>Team Notifications]
        NC2[Email<br/>Escalation Alerts]
        NC3[PagerDuty<br/>On-call Alerts]
        NC4[Webhook<br/>Custom Integrations]
        NC5[SMS<br/>Critical Alerts]
    end
    
    P --> AR
    G --> AR
    L --> AR
    H --> AR
    
    AR --> AG
    AG --> AI
    AI --> AS
    
    AS --> AM
    AM --> AG1
    AM --> AG2
    AM --> AG3
    
    AG1 --> NC1
    AG2 --> NC2
    AG3 --> NC3
    AG3 --> NC4
    AG3 --> NC5
    
    style P fill:#ff6b6b
    style AR fill:#4ecdc4
    style AM fill:#45b7d1
    style NC1 fill:#96ceb4
```

#### **Alert Rule Configuration**

```mermaid
graph LR
    subgraph "Alert Rule Types"
        RT1[High Error Rate<br/>> 5% errors]
        RT2[Slow Response<br/>P95 > 2s]
        RT3[High Latency<br/>P99 > 5s]
        RT4[Service Down<br/>health check failed]
        RT5[High Resource Usage<br/>CPU > 80%]
    end
    
    subgraph "Alert Severity"
        S1[Info<br/>Informational alerts]
        S2[Warning<br/>Attention needed]
        S3[Critical<br/>Immediate action]
        S4[Emergency<br/>Service impact]
    end
    
    subgraph "Alert Actions"
        A1[Notify Team<br/>Slack/Email]
        A2[Page On-call<br/>PagerDuty]
        A3[Auto-remediate<br/>Restart services]
        A4[Escalate<br/>Management notification]
    end
    
    RT1 --> S2
    RT2 --> S2
    RT3 --> S3
    RT4 --> S4
    RT5 --> S3
    
    S1 --> A1
    S2 --> A1
    S3 --> A2
    S4 --> A3
    
    A2 --> A4
    A3 --> A4
    
    style RT1 fill:#ff6b6b
    style S1 fill:#4ecdc4
    style A1 fill:#45b7d1
```

### **Alert Rule Examples**

#### **Prometheus Alert Rules**

```yaml
groups:
  - name: document_management_alerts
    rules:
      # High Error Rate Alert
      - alert: HighErrorRate
        expr: rate(http_errors_total[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
          service: document-api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
      
      # Slow Response Time Alert
      - alert: SlowResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 1m
        labels:
          severity: critical
          service: document-api
        annotations:
          summary: "Slow response time detected"
          description: "95th percentile response time is {{ $value }}s"
      
      # Service Health Alert
      - alert: ServiceUnhealthy
        expr: up{job="document-api"} == 0
        for: 30s
        labels:
          severity: emergency
          service: document-api
        annotations:
          summary: "Service is down"
          description: "Document API service is not responding"
```

#### **Grafana Alert Rules**

```json
{
  "alert": {
    "name": "High Database Latency",
    "message": "Database operations are taking longer than expected",
    "conditions": [
      {
        "type": "query",
        "query": {
          "params": [
            "A",
            "5m",
            "now"
          ]
        },
        "reducer": {
          "type": "avg",
          "params": []
        },
        "evaluator": {
          "type": "gt",
          "params": [1000]
        }
      }
    ],
    "frequency": "1m",
    "handler": 1,
    "message": "Database latency is above threshold",
    "notifications": [
      {
        "uid": "slack-team"
      }
    ]
  }
}
```

---

## üîß Troubleshooting

### **Common Dashboard Issues**

#### **Issue Resolution Flow**

```mermaid
graph TD
    subgraph "Common Issues"
        I1[Dashboard Not Loading]
        I2[Metrics Missing]
        I3[Alerts Not Firing]
        I4[High Dashboard Latency]
        I5[Data Source Errors]
    end
    
    subgraph "Root Causes"
        C1[Network Issues<br/>Connectivity problems]
        C2[Configuration Errors<br/>Wrong endpoints]
        C3[Permission Issues<br/>Access denied]
        C4[Resource Constraints<br/>Memory/CPU limits]
        C5[Data Source Down<br/>Service unavailable]
    end
    
    subgraph "Solutions"
        S1[Check Network<br/>Test connectivity]
        S2[Verify Config<br/>Check settings]
        S3[Check Permissions<br/>User access]
        S4[Increase Resources<br/>Memory/CPU]
        S5[Restart Services<br/>Service recovery]
    end
    
    I1 --> C1
    I2 --> C2
    I3 --> C3
    I4 --> C4
    I5 --> C5
    
    C1 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4
    C5 --> S5
    
    style I1 fill:#ff6b6b
    style C1 fill:#4ecdc4
    style S1 fill:#45b7d1
```

#### **Debugging Commands**

```bash
# Check service status
docker-compose ps

# View service logs
docker-compose logs grafana --tail=50
docker-compose logs prometheus --tail=50
docker-compose logs alertmanager --tail=50

# Test data source connectivity
curl http://localhost:9090/api/v1/query?query=up
curl http://localhost:9200/_cluster/health

# Check Grafana API
curl http://admin:admin123@localhost:3000/api/health

# Verify alert rules
curl http://localhost:9090/api/v1/rules
```

---

## üöÄ Advanced Features

### **Custom Dashboard Development**

#### **Dashboard Development Workflow**

```mermaid
graph LR
    subgraph "Development Phase"
        D1[Requirements<br/>Business needs]
        D2[Design<br/>Panel layout]
        D3[Implementation<br/>Panel creation]
        D4[Testing<br/>Data validation]
    end
    
    subgraph "Deployment Phase"
        DP1[Export<br/>Dashboard JSON]
        DP2[Version Control<br/>Git repository]
        DP3[Automation<br/>CI/CD pipeline]
        DP4[Deployment<br/>Production rollout]
    end
    
    subgraph "Maintenance Phase"
        M1[Monitoring<br/>Dashboard health]
        M2[Updates<br/>Feature additions]
        M3[Optimization<br/>Performance tuning]
        M4[Retirement<br/>End of life]
    end
    
    D1 --> D2
    D2 --> D3
    D3 --> D4
    
    D4 --> DP1
    DP1 --> DP2
    DP2 --> DP3
    DP3 --> DP4
    
    DP4 --> M1
    M1 --> M2
    M2 --> M3
    M3 --> M4
    
    style D1 fill:#ff6b6b
    style DP1 fill:#4ecdc4
    style M1 fill:#45b7d1
```

### **Dashboard Automation**

#### **CI/CD Pipeline for Dashboards**

```mermaid
flowchart TD
    subgraph "Source Control"
        G[Git Repository<br/>Dashboard definitions]
        B[Branch Management<br/>Feature branches]
        PR[Pull Requests<br/>Code review]
    end
    
    subgraph "CI Pipeline"
        T1[Lint Dashboards<br/>JSON validation]
        T2[Test Dashboards<br/>Data source tests]
        T3[Build Artifacts<br/>Dashboard packages]
        T4[Security Scan<br/>Vulnerability check]
    end
    
    subgraph "CD Pipeline"
        D1[Deploy to Staging<br/>Test environment]
        D2[Integration Tests<br/>End-to-end validation]
        D3[Deploy to Production<br/>Live environment]
        D4[Health Checks<br/>Verification]
    end
    
    subgraph "Monitoring"
        M1[Dashboard Health<br/>Uptime monitoring]
        M2[Performance Metrics<br/>Load times]
        M3[Error Tracking<br/>Issue detection]
        M4[User Feedback<br/>Usage analytics]
    end
    
    G --> B
    B --> PR
    PR --> T1
    
    T1 --> T2
    T2 --> T3
    T3 --> T4
    
    T4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4
    
    D4 --> M1
    M1 --> M2
    M2 --> M3
    M3 --> M4
    
    style G fill:#ff6b6b
    style T1 fill:#4ecdc4
    style D1 fill:#45b7d1
    style M1 fill:#96ceb4
```

---

## üåü Conclusion

This observability dashboard setup provides **enterprise-grade monitoring** for the multi-tenant document management system, enabling:

- **Real-time visibility** into system performance and health
- **Proactive alerting** for issue prevention and rapid response
- **Comprehensive metrics** for business and technical analysis
- **Scalable architecture** that grows with your needs
- **Professional dashboards** that support decision-making

### **Next Steps**

1. **Deploy the infrastructure** using the provided configurations
2. **Import the dashboard templates** and customize for your needs
3. **Configure alerting rules** based on your SLAs and requirements
4. **Train your team** on dashboard usage and interpretation
5. **Iterate and improve** based on usage patterns and feedback

---

**Happy Monitoring! üöÄüìäüîç**

*For additional support with dashboard configuration or advanced features, refer to the Grafana documentation or contact your observability team.*
